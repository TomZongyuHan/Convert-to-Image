Index: methods/dataCleanAndNormalize.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Import library and methods\nfrom rpy2.robjects.packages import importr\nimport rpy2.robjects as robjects\nimport numpy as np\n\n\n# Call R file to clean and normalize dataset\n# Input:\n#   filepath: string value, the path of csv file\n#   isRowCount: boolean value,\n#       if dataset is row count, use True\n#       if dataset is not row count, use False\n#   normName: string value, the name of normalization method\n# Output:\n#   normalizedDataset: the variable store normalized dataset\ndef dataCleanAndNormalize(filepath, isRowCount, normName):\n    # Import R's \"base\" package\n    base = importr('base')\n\n    # Import R's \"utils\" package\n    utils = importr('utils')\n\n    # Transform variable 'isRowCount' to String\n    if(isRowCount):\n        isRowCount = 'TRUE'\n    else:\n        isRowCount = 'FALSE'\n\n    # Prepare R variable\n    filepath = '\\\"' + filepath + '\\\"'\n    isRowCount = '\\\"' + isRowCount + '\\\"'\n    normName = '\\\"' + normName + '\\\"'\n\n    # Transform python String to R String\n    robjects.r('''\n        path  <- gsub(\"to\", \"\",''' + filepath + ''')\n        isRowCount <- gsub(\"to\", \"\",''' + isRowCount + ''')\n        normname <- gsub(\"to\", \"\",''' + normName + ''')\n    ''')\n\n    # Install R's packages\n    robjects.r(\"\"\"\n        install.packages(\"BiocManager\", repos = \"http://cran.r-project.org\")\n        BiocManager::install(\"Linnorm\", force = TRUE)\n        BiocManager::install(\"edgeR\", force = TRUE)\n        BiocManager::install(\"scone\", force = TRUE)\n        install.packages(\"Seurat\", repos = \"http://cran.r-project.org\")\n    \"\"\")\n\n    # Import R's packages\n    importr('Linnorm')  # for linnorm\n    importr('scone')  # for scone/TTM/scran\n    importr('edgeR')  # for cpm\n    importr('Seurat')  # for seurat\n\n    robjects.r(\"\"\"\n        # This function used to read, clean and normalize data\n        # Input:\n        #   path: data csv file path\n        #   normName: normalization method name, should be linnorm/scone/ttm/scran/cpm/seurat\n        # Rerutn:\n        #   processed data: rows - genes, columns - cells\n        processDataset <- function(path, isRowCount, normname) {\n          # Read the CSV file and convert it to matrix format\n          rowdata <- read.table(path, sep = \",\", header = FALSE)\n          rowdata <- as.matrix(rowdata)\n    \n          # Determine if the raw data need to transpose\n          if(isRowCount==\"FALSE\"){\n            # Transpose the matrix\n            rowdata <- t(rowdata)\n          }\n    \n          # Delete the rows containing remove names\n          removeNames <- c(\"alpha.contaminated\", \"beta.contaminated\", \"delta.contaminated\", \"Excluded\", \"gamma.contaminated\", \"miss\", \"NA\",\"not applicable\", \"unclassified\", \"unknown\", \"Unknown\", \"zothers\")\n          for (name in removeNames) {\n            rowdata <- rowdata[!grepl(name, rowdata[, 1]), ]\n          }\n    \n          # Save colume name and row name\n          cn <- rowdata[, 1]\n          rn <- rowdata[1, ]\n          rn_c <- \"\"\n          for (name in rn[-1]) {\n            rn_c <- c(rn_c, name)\n          }\n    \n          # Discard first row and first column (row name, column name)\n          rowdata <- rowdata[-1, -1]\n    \n          # Convert character data in the matrix to double data using the mode() method\n          mode(rowdata) <- \"double\"\n    \n          # Check whether NA is still present in the matrix\n          any(is.na(rowdata))\n    \n          # Transpose the matrix, ready for use in the normalization method\n          rowdata <- t(rowdata)\n    \n          # Run different normalize methods accroding to input param\n          if (normname == \"linnorm\") {\n            rowdata <- Linnorm(rowdata, minNonZeroPortion = 0.2)\n          } else if (normname == \"scran\") {\n            rowdata <- SCRAN_FN(rowdata)\n          } else if (normname == \"tmm\") {\n            rowdata <- TMM_FN(rowdata)\n          } else if (normname == \"scone\") {\n            rowdata <- DESEQ_FN(rowdata)\n          } else if (normname == \"cpm\") {\n            rowdata <- cpm(rowdata, log=FALSE)\n          } else if (normname == \"seurat\") {\n            rowdata <- as.matrix(NormalizeData(rowdata))\n          } \n    \n          # Add the rowname and colname of dataset\n          rownames(rowdata) <- rn[-1]\n          colnames(rowdata) <- cn[-1]\n    \n          return(rowdata)\n        }\n    \"\"\")\n\n    robjects.r(\"\"\"\n        res = processDataset(path, isRowCount, normname)\n    \"\"\")\n\n    dataframe = robjects.reval(\"res\")\n    normalized_dataset = np.array(dataframe)\n\n    # Return processed dataset\n    return normalized_dataset\n\n\n# Test\n# filepath = '../originalDatasets/' + 'yan-RowCount.csv'\n# normalized_dataset = dataCleanAndNormalize(filepath, True, \"linnorm\")\n# print(normalized_dataset)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- methods/dataCleanAndNormalize.py	(revision 493adfda65818cc7b449de9a959675da74fccb61)
+++ methods/dataCleanAndNormalize.py	(date 1631163111239)
@@ -39,13 +39,13 @@
     ''')
 
     # Install R's packages
-    robjects.r("""
-        install.packages("BiocManager", repos = "http://cran.r-project.org")
-        BiocManager::install("Linnorm", force = TRUE)
-        BiocManager::install("edgeR", force = TRUE)
-        BiocManager::install("scone", force = TRUE)
-        install.packages("Seurat", repos = "http://cran.r-project.org")
-    """)
+    # robjects.r("""
+    #     install.packages("BiocManager", repos = "http://cran.r-project.org")
+    #     BiocManager::install("Linnorm", force = TRUE)
+    #     BiocManager::install("edgeR", force = TRUE)
+    #     BiocManager::install("scone", force = TRUE)
+    #     install.packages("Seurat", repos = "http://cran.r-project.org")
+    # """)
 
     # Import R's packages
     importr('Linnorm')  # for linnorm
Index: methods/dimensionalityReduce.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Import library and methods\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n\n# Implement and run dimensionality reduction methods\n# Input:\n#   normalizedDataset: the variable store normalized dataset\n#   drName: the name of dimensionality reduction method\n# Output:\n#   drDataset: the variable of datasets after dimensionality reduce\ndef dimensionalityReduce(normalizedDataset, drName):\n    # Transpose the dataset for dimensionality reduction\n    normalizedDatasetT = np.transpose(normalizedDataset)\n\n    # Implement and run dimensionality reduction methods\n    if drName == 'pca':\n        pca = PCA(n_components=0.95)\n        drDatasetT = pca.fit_transform(normalizedDatasetT)\n    elif drName == 'kpca':\n        drDataset = 1\n    elif drName == 'tsne':\n        drDataset = 1\n    elif drName == 'phate':\n        drDataset = 1\n    else:\n        print(\"Please enter a correct normalize name.\")\n\n    # Transpose the dataset for results\n    drDataset = np.transpose(drDatasetT)\n\n    return drDataset\n\n\n# # Test\n# from methods import dataCleanAndNormalize\n#\n# filepath = '../originalDatasets/' + 'yan-RowCount.csv'\n# normalizedDataset = dataCleanAndNormalize.dataCleanAndNormalize(filepath, True, \"linnorm\")\n#\n# drDataset = dimensionalityReduce(normalizedDataset, 'pca')\n# print(drDataset)\n# print(normalizedDataset.shape)\n# print(drDataset.shape)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- methods/dimensionalityReduce.py	(revision 493adfda65818cc7b449de9a959675da74fccb61)
+++ methods/dimensionalityReduce.py	(date 1631163111237)
@@ -33,12 +33,12 @@
 
 
 # # Test
-# from methods import dataCleanAndNormalize
-#
-# filepath = '../originalDatasets/' + 'yan-RowCount.csv'
-# normalizedDataset = dataCleanAndNormalize.dataCleanAndNormalize(filepath, True, "linnorm")
-#
-# drDataset = dimensionalityReduce(normalizedDataset, 'pca')
-# print(drDataset)
-# print(normalizedDataset.shape)
-# print(drDataset.shape)
+from methods import dataCleanAndNormalize
+
+filepath = '../originalDatasets/' + 'yan-RowCount.csv'
+normalizedDataset = dataCleanAndNormalize.dataCleanAndNormalize(filepath, True, "linnorm")
+
+drDataset = dimensionalityReduce(normalizedDataset, 'pca')
+print(drDataset)
+print(normalizedDataset.shape)
+print(drDataset.shape)
